{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File content\n",
    "Tls_wrangling: Code for download and process dataset: search, find, solve\n",
    "TOULOUSE - FRANCE IN OSM\n",
    "Postal codes 31000, 31100, 31200,31300, 31400 and 31500 + CEDEX (those are the postal codes of Toulouse urban area)\n",
    "The Coordinates we will extract are: 43.6552N, 1.5024E, 43.5595S, 1.3935W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of imports we are going to use\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import codecs\n",
    "import json\n",
    "from IPython.display import Image\n",
    "import csv\n",
    "import codecs\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download directly from the OSM page and open the file once downloaded from OSM via API or\n",
    "we can use a functions to download the data from openstreetmap passing as arguments the url and the path and name for our file. \n",
    "The coordinates of the area should be as as West, South, East, North.\n",
    "We include in the fucntion some code to know the size of the file too.\n",
    "We want to know how long it takes the download, we add some code for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the file once downloaded from OSM via API\n",
    "osm_Tls1 = open(\"Tls_Udc_map\", \"r\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to download the map directly form OSM\n",
    "def download_file(api_url, local_filename):\n",
    "    r = requests.get(api_url, stream=True)\n",
    "    with open(local_filename, 'wb') as f:\n",
    "         for chunk in r.iter_content(chunk_size=4096):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "             \n",
    "    file_size=   (os.path.getsize(local_filename))\n",
    "    size_GB=round( file_size/(1024*1024) ,3)\n",
    "    print ('\\nDownload finished. {} is ready.'.format(local_filename))\n",
    "    print (os.path.getsize(local_filename), 'bytes', size_GB, 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download finished. C:/Users/sanchez_sanc/Desktop/data_analyst/Curso/3_Data_Wrangling/UdcT3 is ready.\n",
      "249673100 bytes 238.107 GB\n",
      "Time spend on downloading: 29.0 sec\n"
     ]
    }
   ],
   "source": [
    "#Attention: The coordinates of the area should be as as West, South, East, North.\n",
    "start_time = time.time()\n",
    "#This download the complete Toulouse map\n",
    "url = 'http://overpass-api.de/api/map?bbox=1.3935, 43.5595,1.5024,43.6552'#indicate here the coordinate\n",
    "osm_Tls0 = 'UdcT0'\n",
    "\n",
    "# This download the sample map\n",
    "#url = 'http://overpass-api.de/api/map?bbox=1.4224, 43.6126,1.4354,43.6248'\n",
    "\n",
    "download_file(url, osm_Tls0)\n",
    "\n",
    "time_speend=(time.time() - start_time)\n",
    "\n",
    "print ('Time spend on downloading:', round(time_speend,0),'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extarct an square area but the postal code in France does not delimite a square area, then we need to extract an area that covers that postal code and then filter it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postal codes values and occurrence in download file from Toulouse:\n",
      "\n",
      "[('31000', 1390),\n",
      " ('31200', 429),\n",
      " ('31400', 377),\n",
      " ('31300', 271),\n",
      " ('31500', 269),\n",
      " ('31130', 94),\n",
      " ('31100', 89),\n",
      " ('31240', 17),\n",
      " ('31700', 10),\n",
      " ('31077', 7),\n",
      " ('31027', 5),\n",
      " ('31022', 5),\n",
      " ('31070', 4),\n",
      " ('31055', 4),\n",
      " ('31028', 4),\n",
      " ('31024', 4),\n",
      " ('31079', 3),\n",
      " ('31018', 3),\n",
      " ('31000;31100;31200;31300;31400;31500', 3),\n",
      " ('31432', 2),\n",
      " ('31200\\u200e', 2),\n",
      " ('31062', 2),\n",
      " ('31036', 2),\n",
      " ('31026', 2),\n",
      " ('68199', 1),\n",
      " ('31901', 1),\n",
      " ('31506', 1),\n",
      " ('3140', 1),\n",
      " ('31390', 1),\n",
      " ('31140', 1),\n",
      " ('31081', 1),\n",
      " ('31076', 1),\n",
      " ('31065', 1),\n",
      " ('31060', 1),\n",
      " ('31053', 1),\n",
      " ('31047', 1),\n",
      " ('31037', 1),\n",
      " ('31035', 1),\n",
      " ('31021', 1),\n",
      " ('31020', 1),\n",
      " ('31015', 1),\n",
      " ('31', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Function to investigate the postal code in the map \n",
    "def count_postcodes(filename):\n",
    "    postcodes = {}\n",
    "    for event, elem in ET.iterparse(filename, events=('start', 'end')):\n",
    "        if event == 'end':\n",
    "            key = elem.attrib.get('k')\n",
    "            if key == 'addr:postcode':\n",
    "                postcode = elem.attrib.get('v')\n",
    "                if postcode not in postcodes:\n",
    "                    postcodes[postcode] = 1\n",
    "                else:\n",
    "                    postcodes[postcode] += 1\n",
    "    return postcodes\n",
    "\n",
    "\n",
    "postcodes = count_postcodes(osm_Tls0)\n",
    "sorted_by_occurrence = [(k, v) for (v, k) in sorted([(value, key) for (key, value) in postcodes.items()], reverse=True)]\n",
    "print ('Postal codes values and occurrence in download file from Toulouse:\\n')\n",
    "pprint.pprint(sorted_by_occurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the postal codes and we found more than expected.\n",
    "We check the numbers in https://www.laposte.fr/particulier/outils/trouver-un-code-postal that is the oficial page with the postal code information.\n",
    "We see some codes that could be a mistake :  ('31200\\u200e', should be 31200 then we do not delete but correct it),  ('3140'should be 31400 then we do not delete but correct it), ('68199' should be 31000 then we do not delete but correct it).\n",
    "We found that main of the codes with 1 ocurrency correspond to the system known as CEDEX: Courrier d'Entreprise Ã  Distribution EXceptionnelle (\"business mail with special delivery\"), designed for recipients of large volumes of mail. Those codes are correct as belong to Toulouse\n",
    "We found the codes '31130','31240','31700', '31390', '31140' that we need to eliminate as does not correspond to Toulouse. \n",
    "\n",
    "Now we clean the file to get the perimetre we want to investigate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: to delete the elem that have postal code not in Toulouse area \n",
    "def get_postcode(elem):\n",
    "    if elem.tag in ['node', 'way', 'relation']:\n",
    "        for tag in elem.iter():\n",
    "            if tag.get('k') == 'addr:postcode':\n",
    "                return True, tag.get('v')\n",
    "        return False, None\n",
    "    return False, None\n",
    "\n",
    "def clean_postcode(filename, cleaned_filename):\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for child in ['node', 'way', 'relation']:\n",
    "        for elem in root.findall(child):\n",
    "            has_postcode, postcode_value = get_postcode(elem)\n",
    "            if has_postcode:\n",
    "                if postcode_value in ['31130','31240','31700', '31390', '31140']:\n",
    "                    root.remove(elem)\n",
    "    \n",
    "    return tree.write(cleaned_filename)\n",
    "\n",
    "def count_postcodes(filename):\n",
    "    postcodes = {}\n",
    "    for event, elem in ET.iterparse(filename, events=('start', 'end')):\n",
    "        if event == 'end':\n",
    "            key = elem.attrib.get('k')\n",
    "            if key == 'addr:postcode':\n",
    "                postcode = elem.attrib.get('v')\n",
    "                if postcode not in postcodes:\n",
    "                    postcodes[postcode] = 1\n",
    "                else:\n",
    "                    postcodes[postcode] += 1\n",
    "    return postcodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postcode values and occurrence in Touluse city:\n",
      "\n",
      "[('31000', 1390),\n",
      " ('31200', 429),\n",
      " ('31400', 377),\n",
      " ('31300', 271),\n",
      " ('31500', 269),\n",
      " ('31100', 89),\n",
      " ('31077', 7),\n",
      " ('31027', 5),\n",
      " ('31022', 5),\n",
      " ('31070', 4),\n",
      " ('31055', 4),\n",
      " ('31028', 4),\n",
      " ('31024', 4),\n",
      " ('31079', 3),\n",
      " ('31018', 3),\n",
      " ('31000;31100;31200;31300;31400;31500', 3),\n",
      " ('31432', 2),\n",
      " ('31200\\u200e', 2),\n",
      " ('31062', 2),\n",
      " ('31036', 2),\n",
      " ('31026', 2),\n",
      " ('68199', 1),\n",
      " ('31901', 1),\n",
      " ('31506', 1),\n",
      " ('3140', 1),\n",
      " ('31081', 1),\n",
      " ('31076', 1),\n",
      " ('31065', 1),\n",
      " ('31060', 1),\n",
      " ('31053', 1),\n",
      " ('31047', 1),\n",
      " ('31037', 1),\n",
      " ('31035', 1),\n",
      " ('31021', 1),\n",
      " ('31020', 1),\n",
      " ('31015', 1),\n",
      " ('31', 1)]\n"
     ]
    }
   ],
   "source": [
    "#We clean the postal codes, create a file with right perimetre and get the list of postal codes\n",
    "\n",
    "Tls_0 = 'UdcT2'\n",
    "\n",
    "clean_postcode(osm_Tls0, Tls_0)\n",
    "\n",
    "PC_Tls = count_postcodes(Tls_0)\n",
    "PC_Tls_by_occurrence = [(k, v) for (v, k) in sorted([(value, key) for (key, value) in PC_Tls.items()], reverse=True)]\n",
    "\n",
    "print ('Postcode values and occurrence in Touluse city:\\n')\n",
    "pprint.pprint(PC_Tls_by_occurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file now have the right perimetre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to analize if we have issues in the data before to transfer them to a database for our analysis and what kind of data we have in the file ex. tags, nodes, members.\n",
    "We use a function to count the tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start checking the \"k\" value for each \"tag\" and see if there are any potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to check if issues in 'k' value for 'tag'\n",
    "# Focus on lower case, tag with underscore or lower_colon), problematic characters\n",
    "# as: ?, %, #, $, @ and other posible characteres\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if lower.search(element.attrib['k']):\n",
    "            keys['lower'] += 1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "\n",
    "    return keys \n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 587020, 'lower_colon': 157334, 'other': 12933, 'problemchars': 1}\n"
     ]
    }
   ],
   "source": [
    "# We pass the function to Tls_0\n",
    "keys = process_map(Tls_0)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found only one problem. It seams that the quality of this data set is very good \n",
    "Now lets find the problem and solve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to get the element that generate the problem\n",
    "def get_problemkeys(filename):\n",
    "    \"\"\"\n",
    "    Takes in a dataset in XML format, parses it and returns a list with the values of tags with problematic characters.\n",
    "    \"\"\"\n",
    "    problemchars_list = []\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == 'tag':\n",
    "            if problemchars.search(element.attrib['k']):\n",
    "                problemchars_list.append(element.attrib['k'])\n",
    "    return problemchars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Online order']\n"
     ]
    }
   ],
   "source": [
    "# We pass the function to Tls_0\n",
    "print(get_problemkeys(Tls_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is not and issue as the point is an space between two words. We do not need to correct it.\n",
    "Now we analyse the nodes, tag, members ... of the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to show the tags and how many of them\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(filename, events=('start', )):\n",
    "        if elem.tag not in tags:\n",
    "            tags[elem.tag] = 1\n",
    "        else:\n",
    "            tags[elem.tag] += 1\n",
    "    return tags\n",
    "\n",
    "#Funtion to get the attributes and their amounts\n",
    "def count_attrs(filename):\n",
    "    attrs = {}\n",
    "    for event, elem in ET.iterparse(filename, events=('start', 'end')):\n",
    "        if event == 'end':\n",
    "            for attr in elem.attrib:\n",
    "                if attr not in attrs:\n",
    "                    attrs[attr] = 1\n",
    "                else:\n",
    "                    attrs[attr] += 1\n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element types and occurrence of right perimetre :\n",
      "\n",
      "[('nd', 1291481),\n",
      " ('node', 956186),\n",
      " ('tag', 757363),\n",
      " ('member', 178950),\n",
      " ('way', 160279),\n",
      " ('relation', 4695),\n",
      " ('osm', 1),\n",
      " ('note', 1),\n",
      " ('meta', 1),\n",
      " ('bounds', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Iterative parsing to process the file with right perimetre Tls_0\n",
    "\n",
    "tags_Tls = count_tags(Tls_0)\n",
    "tags_Tls_by_occurrence = [(k, v) for (v, k) in sorted([(value, key) for (key, value) in tags_Tls.items()], reverse=True)]\n",
    "\n",
    "print ('Element types and occurrence of Toulouse city map: \\n')\n",
    "pprint.pprint(tags_Tls_by_occurrence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make now a view of atribures on the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes and occurrence on Toulouse data:\n",
      "\n",
      "[('ref', 1470431),\n",
      " ('version', 1121161),\n",
      " ('user', 1121160),\n",
      " ('uid', 1121160),\n",
      " ('timestamp', 1121160),\n",
      " ('id', 1121160),\n",
      " ('changeset', 1121160),\n",
      " ('lon', 956186),\n",
      " ('lat', 956186),\n",
      " ('v', 757363),\n",
      " ('k', 757363),\n",
      " ('type', 178950),\n",
      " ('role', 178950),\n",
      " ('osm_base', 1),\n",
      " ('minlon', 1),\n",
      " ('minlat', 1),\n",
      " ('maxlon', 1),\n",
      " ('maxlat', 1),\n",
      " ('generator', 1)]\n",
      "Time spend on running: 22.0 sec\n"
     ]
    }
   ],
   "source": [
    "# We pass the functions to Tls_0 and we calculate the time to pass this, as big database take time to pass\n",
    "start_time1 = time.time()\n",
    "attrs = count_attrs(Tls_0)\n",
    "sorted_by_occurrence = [(k, v) for (v, k) in sorted([(value, key) for (key, value) in attrs.items()], reverse=True)]\n",
    "\n",
    "print ('Attributes and occurrence on Toulouse city map:\\n')\n",
    "pprint.pprint(sorted_by_occurrence)\n",
    "\n",
    "time_speend1=(time.time() - start_time1)\n",
    "print ('Time spend on running:', round(time_speend1,0),'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could check now the contributors to the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtions to get how many of uids and users \n",
    "\n",
    "def process_map_uid(filename):\n",
    "    uids = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'uid' in element.attrib :\n",
    "            uids.add(element.attrib['uid'])\n",
    "    return uids\n",
    "\n",
    "def process_map_user(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'user' in element.attrib :\n",
    "            users.add(element.attrib['user'])\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pass the functions to Tls_0\n",
    "T_uids=process_map_uid(Tls_0)\n",
    "T_users=process_map_user(Tls_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1284 contributor number 1284 contributor Nickname\n"
     ]
    }
   ],
   "source": [
    "# We see the amount of contributors in Tls_0: ids and name \n",
    "print(len(T_uids),'contributor number',len(T_users),'contributor Nickname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of contributors is the same for uid and user, we do not see any issue on the user view.\n",
    "We could get the information for 'uid' or 'user'. The uid never change but the contributors could decide to change their 'user' (kind of nickname). Existing elements will reflect the new user name without needing any version change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each contributor is a person that spend time to contribure to the map, let see how many contributors are and how many contributions, and the ratio of contributions per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtions to get information about the contributors\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'uid' in element.attrib :\n",
    "            users.add(element.attrib['uid'])\n",
    "    return users\n",
    "\n",
    "#Funtion to get the name (Nickmane) \n",
    "def count_name_users(filename):\n",
    "    many_name_users = {}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'user' in element.attrib :\n",
    "            many_name_user  = element.attrib.get('user')\n",
    "            if many_name_user not in many_name_users:\n",
    "                many_name_users[many_name_user] = 1\n",
    "            else:\n",
    "                many_name_users[many_name_user] += 1\n",
    "                \n",
    "    return many_name_users\n",
    "\n",
    "#Funtion to get the amount of contribution by name (Nickmane) \n",
    "def count_rib(filename):\n",
    "    cont=0\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'user' in element.attrib :\n",
    "            cont= cont+1\n",
    "    return cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1284 contributors and 1121160 cotributions\n",
      "873.0 contribution per contribtor as average\n"
     ]
    }
   ],
   "source": [
    "# We pass the function to Tls_0\n",
    "\n",
    "name_users = count_name_users(Tls_0)\n",
    "num_users = count_rib(Tls_0)\n",
    "\n",
    "print(len(name_users),'contributors and',num_users,'cotributions')\n",
    "print(round((num_users/(len(name_users))),0),'contribution per contribtor as average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check now the names of the \"street\". France street name start with the type of street and then the name.\n",
    "We need to take that into acount for the expresion to be included in the function for street type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get the type of street \n",
    "\n",
    "street_type_re = re.compile(r'(?P<word>)(\\b\\w+\\b)', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d, expression):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print (expression % (k, v))\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def check_street(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "    print(street_types, \"%s: %d\")\n",
    "    return(street_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Rue': 5679, 'AllÃ©e': 174, 'Avenue': 623, 'Grande': 160, 'Place': 574, 'Impasse': 226, 'Route': 421, 'Chemin': 331, 'Boulevard': 827, 'AllÃ©es': 110, 'Esplanade': 18, 'route': 2, 'Angle': 11, 'face': 1, 'Sur': 2, 'rue': 22, 'Port': 9, '6': 1, 'AVENUE': 2, 'avenue': 9, 'place': 2, 'Promenade': 6, 'Quai': 36, 'Passage': 78, 'Cheminement': 24, 'Voie': 8, 'ROUTE': 1, 'Descente': 1, 'Square': 3, 'Lotissement': 1, 'allÃ©es': 1, '9': 1, 'allÃ©e': 1, 'Contre': 1, 'Cours': 22, 'Parvis': 1, '107': 1, 'chemin': 2, 'FrÃ©dÃ©ric': 1, 'voie': 1, 'PÃ©riphÃ©rique': 8}) %s: %d\n"
     ]
    }
   ],
   "source": [
    "# We pass the functions to Tls_0 and check the type of street\n",
    "all_types = check_street(Tls_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the rigth names of street that we will use to create our expected values list, we have Capitals/non capitals and there are other types that we need to investigate as seam not right. \n",
    "\n",
    "Remark:\n",
    "'AllÃ©e' and 'AllÃ©es' are both correct \n",
    "\n",
    "We create a list of mapping to correct Capitals/non capitals: 'route','rue', 'AVENUE', 'avenue', 'place', 'ROUTE', 'allÃ©es', 'allÃ©e', 'voie', 'PÃ©riphÃ©rique'\n",
    "\n",
    "We investigate for the wrong naming: 'Grande', 'face', 'Sur','107', 'FrÃ©dÃ©ric', '6', '9', 'Angle'Lotissement'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to audit the street names and investigate that seams not right\n",
    "\n",
    "street_type_re = re.compile(r'(?P<word>)(\\b\\w+\\b)', re.IGNORECASE)\n",
    "\n",
    "expected = ['Rue', 'AllÃ©e', 'Avenue',  'Place', 'Impasse', 'Route', 'Chemin', 'Boulevard', \n",
    "            'AllÃ©es', 'Esplanade', 'Port', 'Promenade', 'Quai', 'Passage', 'Cheminement', \n",
    "            'Voie', 'Descente', 'Square', 'Contre-AllÃ©e', 'Cours', 'Parvis','PÃ©riphÃ©rique']\n",
    "\n",
    "mapping = {'route' :'Route',\n",
    "            'ROUTE' :'Route',\n",
    "            'rue' : 'Rue' ,\n",
    "            'AVENUE': 'Avenue',\n",
    "            'avenue': 'Avenue',\n",
    "            'place':  'Place' ,\n",
    "            'allÃ©es' :'AllÃ©es',\n",
    "            'allÃ©e': 'AllÃ©e',\n",
    "            'voie' :'Voie',\n",
    "           'chemin':'Chemin'\n",
    "                   }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(file):\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'107': {'107'},\n",
      " '6': {'6 Impasse Leonce Couture'},\n",
      " '9': {'9'},\n",
      " 'AVENUE': {'AVENUE JEAN RIEUX'},\n",
      " 'Angle': {'Angle 1 rue du Taur et 18 place du Capitole',\n",
      "           'Angle 1 rue temponiÃ¨res et 2 rue Saint-Ursule',\n",
      "           'Angle 10 place Wilson et 39 rue Lafayette',\n",
      "           \"Angle 14 rue d'Austerlitz et 9 Boulevard de Strasbourg\",\n",
      "           \"Angle 18 place Wilson et 2 rue d'Austerlitz\",\n",
      "           'Angle 21 rue Croix Baragnon et rue Tolosane',\n",
      "           'Angle 33 rue Lafayette et 1 rue du Rempart Villeneuve',\n",
      "           'Angle 34 rue de Metz et 9 rue des Arts',\n",
      "           'Angle 6 place Wilson et rue Saint-Antoine du T',\n",
      "           'Angle 8 place Wilson et 13 rue Lapeyrouse',\n",
      "           'Angle parking et rue du Rempart Villeneuve'},\n",
      " 'Contre': {\"Contre-AllÃ©e du Boulevard de l'Embouchure\"},\n",
      " 'FrÃ©dÃ©ric': {'FrÃ©dÃ©ric Petit'},\n",
      " 'Grande': {'Grande Rue Saint-Michel', 'Grande Rue Nazareth'},\n",
      " 'Lotissement': {'Lotissement Futuropolis - Impasse RenÃ© Mouchotte'},\n",
      " 'ROUTE': {'ROUTE DE BLAGNAC'},\n",
      " 'Sur': {'Sur facade du ThÃ©Ã¢tre face 1 place du Capitole',\n",
      "         'Sur parking face Ã  la rue Porte Sardane'},\n",
      " 'allÃ©e': {'allÃ©e Fernand Jourdant'},\n",
      " 'allÃ©es': {'allÃ©es Jules-Guesde'},\n",
      " 'avenue': {'avenue Etienne Billieres',\n",
      "            'avenue Jean-Rieux',\n",
      "            'avenue de Lombez',\n",
      "            'avenue jean-chaubet'},\n",
      " 'chemin': {'chemin de la Salade Ponsan', 'chemin Cassaing'},\n",
      " 'face': {'face 5 place du Capitole'},\n",
      " 'place': {'place Saint-Cyprien', 'place Papus'},\n",
      " 'route': {'route de launaguet', 'route de Fronton'},\n",
      " 'rue': {'rue Baour-Lormian',\n",
      "         'rue BÃ©nezet',\n",
      "         'rue Reclusane',\n",
      "         'rue Saint-Lys',\n",
      "         'rue ThÃ©odore Lenotre',\n",
      "         'rue de Cugnaux',\n",
      "         \"rue de l'ayga\",\n",
      "         \"rue de l'industrie\",\n",
      "         'rue de la Nive',\n",
      "         'rue des frÃ¨res Lion',\n",
      "         'rue du Colonel Driant',\n",
      "         'rue henri jansou',\n",
      "         'rue paule raymondis'},\n",
      " 'voie': {'voie du T.O.E.C.'}}\n"
     ]
    }
   ],
   "source": [
    "# We pass the fuctions to see the details of not expected names\n",
    "st_types = audit(Tls_0)\n",
    "pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After investigation we find that some street names are correct and other need to be change\n",
    "We will update our expected value list with the names that are correct, the mapping list too if needed and we will create a change list for the changes we want to implement.\n",
    "We will procede like this: \n",
    "1. 'Grande' is correct we correct our expected values list\n",
    "2. '6 Impasse Leonce Couture' should become ' Impasse Leonce Couture, 6' \n",
    "3. 'voie du T.O.E.C.' is correct we correct our expected values list and the mapping list\n",
    "4. 'Lotissement': {'Lotissement Futuropolis - Impasse RenÃ© Mouchotte'},should be Impasse RenÃ© Mouchotte_Lotissement Futuropolis \n",
    "5. 'FrÃ©dÃ©ric': {'FrÃ©dÃ©ric Petit'}, should be Rue FrÃ©dÃ©ric Petit\n",
    "6. 107  should be Cours Rosalind Franklin, 107\n",
    "7. 9 should be Rue Reclusane, 9 \n",
    "8. Angle adress refers to the places where videocam are placed. We will not correct that in our file\n",
    "9. 'Sur': {'Sur facade du ThÃ©Ã¢tre face 1 place du Capitole','Sur parking face Ã  la rue Porte Sardane'}, same than case 8\n",
    "10. 'face': {'face 5 place du Capitole'} same than 8\n",
    "11. to avoid error we pass angle, Sur and face as expected values  \n",
    "\n",
    "We will use those informations later when preparing for CSV creation\n",
    "Lets continue now checking the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate now a bit on the amenities - tag k=\"amenity\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get the type of amenity \n",
    "\n",
    "amenity_type_re = re.compile(r'([\\w.-]+)', re.IGNORECASE)\n",
    "amenity_types = defaultdict(int)\n",
    "\n",
    "def audit_amenity_type(amenity_types, amenity_name):\n",
    "    m = amenity_type_re.search(amenity_name)\n",
    "    if m:\n",
    "        amenity_type = m.group()\n",
    "        amenity_types[amenity_type] += 1\n",
    "\n",
    "def print_sorted_dict(d, expression):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print (expression % (k, v))\n",
    "\n",
    "def is_amenity_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"amenity\")\n",
    "\n",
    "def check_amenity(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_amenity_name(elem):\n",
    "            audit_amenity_type(amenity_types, amenity.attrib['v'])\n",
    "    print(amenity_types, \"%s: %d\")\n",
    "    return(amenity_types)\n",
    "\n",
    "def check_amenity(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_amenity_name(elem):\n",
    "            audit_amenity_type(amenity_types, elem.attrib['v'])\n",
    "    print(amenity_types, \"%s: %d\")\n",
    "    return(amenity_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'theatre': 44, 'fuel': 37, 'drinking_water': 180, 'motorcycle_parking': 76, 'pharmacy': 164, 'parking': 1397, 'atm': 122, 'post_office': 44, 'post_box': 279, 'bank': 172, 'nightclub': 31, 'police': 15, 'restaurant': 755, 'recycling': 901, 'toilets': 89, 'place_of_worship': 94, 'school': 270, 'bicycle_parking': 699, 'bicycle_rental': 287, 'kindergarten': 212, 'pub': 58, 'townhall': 27, 'cinema': 9, 'bar': 174, 'fountain': 74, 'car_wash': 34, 'fast_food': 363, 'cafe': 172, 'parking_entrance': 138, 'telephone': 2, 'taxi': 6, 'bench': 994, 'bbq': 2, 'arts_centre': 27, 'library': 36, 'waste_basket': 443, 'music_venue': 1, 'swingerclub': 1, 'car_rental': 10, 'veterinary': 13, 'social_facility': 111, 'doctors': 91, 'casino': 1, 'vending_machine': 187, 'marketplace': 40, 'ice_cream': 13, 'shelter': 70, 'dentist': 118, 'parking_space': 1236, 'company': 1, 'driving_school': 42, 'embassy': 2, 'clock': 4, 'clinic': 4, 'toy_library': 12, 'college': 59, 'mobile_library': 19, 'community_centre': 46, 'bus_station': 6, 'music_school': 5, 'car_sharing': 21, 'charging_station': 12, 'courthouse': 7, 'bureau_de_change': 4, 'gym': 1, 'photo_booth': 3, 'hookah_lounge': 1, 'money_transfer': 2, 'personal_service': 1, 'childcare': 2, 'stripclub': 2, 'bicycle_repair_station': 1, 'university': 63, 'ticket_validator': 4, 'public_building': 7, 'apartments': 3, 'dojo': 14, 'smoking_area': 2, 'piano': 1, 'waste_disposal': 60, 'social_centre': 5, 'public_bookcase': 12, 'studio': 3, 'water_point': 3, 'music': 1, 'vehicle_inspection': 8, 'internet_cafe': 1, 'hospital': 19, 'language_school': 3, 'gambling': 1, 'dance_school': 1, 'research_institute': 3, 'fire_station': 2, 'dancing_school': 3, 'watering_place': 1, 'heavy-equipment_rental': 1, 'conference_centre': 1, 'monastery': 1, 'animal_shelter': 1, 'planetarium': 1, 'food_court': 1, 'mist_spraying_cooler': 1, 'reception_desk': 1}) %s: %d\n"
     ]
    }
   ],
   "source": [
    "# We pass the functions to Tls_0 and check the type of street\n",
    "all_types = check_amenity(Tls_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems correct and the names are in line with OSM standard\n",
    "We will see now the shops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('clothes', 440),\n",
      " ('hairdresser', 279),\n",
      " ('bakery', 185),\n",
      " ('convenience', 162),\n",
      " ('beauty', 112),\n",
      " ('butcher', 104),\n",
      " ('jewelry', 101),\n",
      " ('supermarket', 93),\n",
      " ('vacant', 91),\n",
      " ('shoes', 87),\n",
      " ('optician', 86),\n",
      " ('newsagent', 72),\n",
      " ('deli', 70),\n",
      " ('books', 63),\n",
      " ('car_repair', 61),\n",
      " ('laundry', 52),\n",
      " ('florist', 48),\n",
      " ('mobile_phone', 47),\n",
      " ('cosmetics', 43),\n",
      " ('interior_decoration', 42),\n",
      " ('bicycle', 42),\n",
      " ('travel_agency', 39),\n",
      " ('greengrocer', 38),\n",
      " ('car', 38),\n",
      " ('alcohol', 36),\n",
      " ('confectionery', 28),\n",
      " ('tattoo', 26),\n",
      " ('dry_cleaning', 24),\n",
      " ('tobacco', 23),\n",
      " ('art', 23),\n",
      " ('sports', 22),\n",
      " ('furniture', 22),\n",
      " ('motorcycle', 21),\n",
      " ('pastry', 19),\n",
      " ('ticket', 18),\n",
      " ('fabric', 18),\n",
      " ('hearing_aids', 16),\n",
      " ('gift', 16),\n",
      " ('doityourself', 16),\n",
      " ('copyshop', 16),\n",
      " ('cheese', 15),\n",
      " ('bag', 15),\n",
      " ('antiques', 15),\n",
      " ('seafood', 14),\n",
      " ('musical_instrument', 14),\n",
      " ('stationery', 13),\n",
      " ('frozen_food', 13),\n",
      " ('convenience;gas', 13),\n",
      " ('yes', 12),\n",
      " ('toys', 12),\n",
      " ('kiosk', 12),\n",
      " ('estate_agent', 12),\n",
      " ('computer', 12),\n",
      " ('boutique', 12),\n",
      " ('massage', 11),\n",
      " ('garden_centre', 11),\n",
      " ('electronics', 11),\n",
      " ('chocolate', 11),\n",
      " ('variety_store', 10),\n",
      " ('e-cigarette', 10),\n",
      " ('wine', 9),\n",
      " ('paint', 9),\n",
      " ('music', 9),\n",
      " ('kitchen', 9),\n",
      " ('tea', 8),\n",
      " ('medical_supply', 8),\n",
      " ('mall', 8),\n",
      " ('kitchenware', 8),\n",
      " ('hifi', 8),\n",
      " ('craft', 8),\n",
      " ('baby_goods', 8),\n",
      " ('photo', 7),\n",
      " ('storage_rental', 6),\n",
      " ('locksmith', 6),\n",
      " ('leather', 6),\n",
      " ('household_linen', 6),\n",
      " ('games', 6),\n",
      " ('car_parts', 6),\n",
      " ('watches', 5),\n",
      " ('video_games', 5),\n",
      " ('radiotechnics', 5),\n",
      " ('hardware', 5),\n",
      " ('fashion_accessories', 5),\n",
      " ('erotic', 5),\n",
      " ('coffee', 5),\n",
      " ('second_hand', 4),\n",
      " ('perfumery', 4),\n",
      " ('money_lender', 4),\n",
      " ('gold', 4),\n",
      " ('gas', 4),\n",
      " ('chemist', 4),\n",
      " ('carpet', 4),\n",
      " ('video', 3),\n",
      " ('tyres', 3),\n",
      " ('trade', 3),\n",
      " ('nutrition_supplements', 3),\n",
      " ('knife', 3),\n",
      " ('health_food', 3),\n",
      " ('funeral_directors', 3),\n",
      " ('farm', 3),\n",
      " ('department_store', 3),\n",
      " ('bed', 3),\n",
      " ('tailor', 2),\n",
      " ('shoe_repair', 2),\n",
      " ('pet', 2),\n",
      " ('party', 2),\n",
      " ('motorcycle_repair', 2),\n",
      " ('mobility_scooter', 2),\n",
      " ('ice_cream', 2),\n",
      " ('herbalist', 2),\n",
      " ('hairdresser_supply', 2),\n",
      " ('general', 2),\n",
      " ('fireplace', 2),\n",
      " ('dairy', 2),\n",
      " ('curtain', 2),\n",
      " ('communication', 2),\n",
      " ('charity', 2),\n",
      " ('candles', 2),\n",
      " ('auction_house', 2),\n",
      " ('anime', 2),\n",
      " ('weapons', 1),\n",
      " ('water', 1),\n",
      " ('traiteur', 1),\n",
      " ('tool_hire', 1),\n",
      " ('tiles', 1),\n",
      " ('spices', 1),\n",
      " ('spare_parts', 1),\n",
      " ('sewing', 1),\n",
      " ('security', 1),\n",
      " ('printer_ink', 1),\n",
      " ('printer_cartridges', 1),\n",
      " ('pet_grooming', 1),\n",
      " ('outdoor', 1),\n",
      " ('orthopedics', 1),\n",
      " ('newsagent;tobacco', 1),\n",
      " ('lighting', 1),\n",
      " ('houseware', 1),\n",
      " ('hat', 1),\n",
      " ('handicraft', 1),\n",
      " ('frame', 1),\n",
      " ('florist;pet;garden_centre', 1),\n",
      " ('ephemera', 1),\n",
      " ('electrical', 1),\n",
      " ('computer_repair', 1),\n",
      " ('collector', 1),\n",
      " ('camera', 1),\n",
      " ('cafe', 1),\n",
      " ('bookmaker', 1),\n",
      " ('beverages', 1),\n",
      " ('battery', 1),\n",
      " ('bathroom_furnishing', 1),\n",
      " ('barber_shop', 1),\n",
      " ('astronomy', 1),\n",
      " ('alcohol;butcher;cheese', 1),\n",
      " ('agrarian', 1)]\n"
     ]
    }
   ],
   "source": [
    "# This show the street by type\n",
    "\n",
    "Shops_type_re = re.compile(r'([\\w.-]+)', re.IGNORECASE)\n",
    "\n",
    "def is_Shops_name(elem):\n",
    "    return (elem.attrib['k'] == \"shop\")\n",
    "def audit_shops(filename):\n",
    "    \n",
    "    shops = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_Shops_name(tag):\n",
    "                    shop = tag.attrib['v']\n",
    "                    if shop not in shops:\n",
    "                        shops[shop] = 1\n",
    "                    else:\n",
    "                        shops[shop ] += 1\n",
    "                \n",
    "    return shops\n",
    "                    \n",
    "\n",
    "shops_types = audit_shops(Tls_0)\n",
    "sorted_by_Shops = [(k, v) for (v, k) in sorted([(value, key) for (key, value) in shops_types.items()], reverse=True)]\n",
    "pprint.pprint(sorted_by_Shops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shop seams right but we want to investigate 'yes' and 'convenience;gas'. \n",
    "We check that in OSM wiki and both are validated with some coments. We do not need to correct them\n",
    "\n",
    "After those investigations we are ready to parse the file to CSV. We will do that in another document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
